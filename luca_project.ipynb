{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "887edaa3",
   "metadata": {},
   "source": [
    "# How does satisfying group and individual fairness notions affect the performance of machine learning models and how do they compare?\n",
    "In this section we will explore how group and individual fairness notions affect the performance of a machine learning model. First, we motivate this research questions by contextualizing its relevance in problems of machine learning. We discuss each notion separately. Then, we briefly look at the theoretic background of these notions and predict the expected performance. We conclude by putting these notions to the test."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b83aa71",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Machine learning applications are increasingly being applied in the industry. Legislators, insurers and banks are playing catch-up to integrate this technology in the process of decision-making. Supervised learning often relies on historical data. This means that bias present in the data is transferred to the model. Perpetuating this bias is not only unfair, but often unlawful or contrary to company policy. Chouldechova & Roth (2018) identify three causes of unfairness:\n",
    "* **Bias in training data**: historical data that has human bias embedded in it. A classic example is the disproportionate amount of crime committed by some marginalized and ostracized communities. However, this can often be explained by considering the socio-economic situation. Also, these areas might be policed at a higher rate, which further skews crime-prediction models.\n",
    "* **Minimizing average error**: a majority group will be more accurately represented in a model than a minority group. Naturally, it follows from the fact that the majority group has a larger representation and thus minimizing errors will benefit more if the error of each individual has the same weight.\n",
    "* **Related to exploration**: online learning models that gets updated with new information while being used, can greatly benefit from the information gained of taking suboptimal decisions. This can be either amoral (e.g. for medical procedures) or benefit/disadvantage certain groups.\n",
    "\n",
    "Different fairness notions have been introduced in the literature to mitigate problems like these. Knowing which notion to satisfy, depends on the problem at hand."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08e0e741",
   "metadata": {},
   "source": [
    "## Fairness notions\n",
    "We will now introduce each fairness notion that is going to be applied."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01c1d719",
   "metadata": {},
   "source": [
    "### Statistical parity\n",
    "There exists different group fairness notions (for a full list, see Barocas et al., 2017). Group fairness notions seek to be fair for a protected group. In this work, we only consider **statistical parity** (also known as **demographic parity**). It is satisfied for a given (sensitive) group attribute when the *positive* classification distribution for each group is identical to that of the entire population (Barocas et al., 2017). This means that predictions needs to be statistically independent with respect to the group attribute. For a group attribute $G$ and attribute to predict $Y$ (with \"$+$\" deemed a *positive* prediction), it must hold that:\n",
    "$$\\forall a,b \\in G: \\mathbb{P}(Y = + \\mid G = a) = \\mathbb{P}(Y = + \\mid G = b)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29126d0f",
   "metadata": {},
   "source": [
    "### Individual fairness\n",
    "As opposed to group fairness notions, **individual fairness** does not look at a sensitive property in particular. Individual fairness is satisfied when similar individuals are classified similarly, in proportion to their degree of similarity (Dwork et al., 2012). To accomplish this, we need a distance metric $d$ to quantify similarity between individuals and a distance metric $D$ to compare the difference between distributions. Individual fairness is satisfied for a classifier $M$ if it satisfies the $(D, d)$-**Lipschitz property**. It entails that for any two individuals $x$, $y$ it holds that:\n",
    "$$D(M(x), M(y)) \\le d(x, y)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24c96e63",
   "metadata": {},
   "source": [
    "## Sources\n",
    "* Chouldechova, A., & Roth, A. (2018). The frontiers of fairness in machine learning. *arXiv preprint arXiv:1810.08810*.\n",
    "* Barocas, S., Hardt, M., & Narayanan, A. (2017). Fairness in machine learning. Nips tutorial, 1, 2017.\n",
    "* Dwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2012, January). Fairness through awareness. In *Proceedings of the 3rd innovations in theoretical computer science conference* (pp. 214-226)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95b66d00",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
