{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "887edaa3",
   "metadata": {},
   "source": [
    "# How do different fairness notions affect the performance of machine learning models?\n",
    "In this section we will explore how different fairness notions affect the performance of a machine learning model. We will cover the following notions:\n",
    "* **Statistical parity**\n",
    "* **Individual fairness**\n",
    "* **Equalized odds**\n",
    "* **Equal opportunity**\n",
    "\n",
    "Barocas et al. (2017) classify many more fairness notions.\n",
    "\n",
    "First, we motivate the relevance of this research questions by contextualizing it in contemporary trends of machine learning. We discuss each notion separately. Then, we briefly look at the theoretic background of these notions and predict the expected performance. We conclude by putting these notions to the test."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b83aa71",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Machine learning applications are increasingly being applied in the industry. Legislators, insurers and banks are playing catch-up to integrate this technology in the process of decision-making. Supervised learning often relies on historical data. This means that bias present in the data is transferred to the model. Perpetuating this bias is not only unfair, but often unlawful or contrary to company policy. Chouldechova & Roth (2018) identify three causes of unfairness:\n",
    "* **Bias in training data**: historical data that has human bias embedded in it. A classic example is the disproportionate amount of crime committed by some marginalized and ostracized communities. However, this can often be explained by considering the socio-economic situation. Also, these areas might be policed at a higher rate, which further skews crime-prediction models.\n",
    "* **Minimizing average error**: a majority group will be more accurately represented in a model than a minority group. Naturally, it follows from the fact that the majority group has a larger representation and thus minimizing errors will benefit more if the error of each individual has the same weight.\n",
    "* **Related to exploration**: online learning models that gets updated with new information while being used, can greatly benefit from the information gained of taking suboptimal decisions. This can be either amoral (e.g. for medical procedures) or benefit/disadvantage certain groups.\n",
    "\n",
    "Model with a low VC-dimension (and thus high bias) will amplify these undesired effects."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08e0e741",
   "metadata": {},
   "source": [
    "## Fairness notions\n",
    "We will now introduce the different fairness notions that are going to be applied."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01c1d719",
   "metadata": {},
   "source": [
    "### Group fairness\n",
    "**Statistical parity** or **demographic parity** is a group fairness notion. It is satisfied for a given (sensitive) group attribute when the *positive* classification distribution for each group is identical to that of the entire population (Barocas et al., 2017). This means that predictions needs to be statistically independent with respect to the group attribute. For a group attribute $G$ and attribute to predict $R$ (with \"$+$\" deemed a *positive* prediction), it holds that:\n",
    "$$\\forall a,b \\in G: \\mathbb{P}(R = + \\mid G = a) = \\mathbb{P}(R = + \\mid G = b)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29126d0f",
   "metadata": {},
   "source": [
    "### Individual fairness"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a771776c",
   "metadata": {},
   "source": [
    "### Equalized odds\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "107b6552",
   "metadata": {},
   "source": [
    "### Equal opportunity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24c96e63",
   "metadata": {},
   "source": [
    "## Sources\n",
    "* Chouldechova, A., & Roth, A. (2018). The frontiers of fairness in machine learning. *arXiv preprint arXiv:1810.08810*.\n",
    "* Barocas, S., Hardt, M., & Narayanan, A. (2017). Fairness in machine learning. Nips tutorial, 1, 2017.\n",
    "* Dwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2012, January). Fairness through awareness. In *Proceedings of the 3rd innovations in theoretical computer science conference* (pp. 214-226)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95b66d00",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
